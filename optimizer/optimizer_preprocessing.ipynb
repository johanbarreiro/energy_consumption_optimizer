{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f673c4191a060be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:02:31.879642Z",
     "start_time": "2024-07-09T21:02:31.874848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\OneDrive\\Documents\\IE\\3. Trimestre\\Venture Lab & Capstone\\Capstone\\Tech side\\vl_optimizer\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ab01f",
   "metadata": {},
   "source": [
    "## Create a merged dataframe with the required information for the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b49497",
   "metadata": {},
   "source": [
    "Import the forecasted data and add it to merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:02:32.336126Z",
     "start_time": "2024-07-09T21:02:31.880973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import data.data_utils as du\n",
    "\n",
    "def join_csv_on_time_column(directory='.'):\n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    \n",
    "    # Initialize an empty dataframe\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each CSV file and merge them on the 'Time' column\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "        \n",
    "        df = df[['Time', 'Forecasting Values']]\n",
    "        \n",
    "        column_name = csv_file[:-21]\n",
    "        \n",
    "        df[column_name] = df['Forecasting Values']\n",
    "        \n",
    "        df = df.drop('Forecasting Values', axis=1)  \n",
    "        \n",
    "        # Check if 'Time' column exists in the dataframe\n",
    "        if 'Time' not in df.columns:\n",
    "            print(f\"'Time' column not found in {csv_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Merge the dataframes\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='Time', how='outer')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Run the function and store the result\n",
    "merged_dataframe = join_csv_on_time_column(directory='data/forecasted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa0588",
   "metadata": {},
   "source": [
    "Add the data from the processed dataset and adapt the naming of certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d384f9fa9ac527f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:02:32.380218Z",
     "start_time": "2024-07-09T21:02:32.336822Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('data/processed_data/industrial_sites_processed/2024-07-09T19-25-47_industrial_site2_processed.csv')\n",
    "merged_dataframe = pd.merge(merged_dataframe, processed_df[['Time', 'heat_index', 'price_mWh']], on='Time', how='inner')\n",
    "merged_dataframe['Hour'] = merged_dataframe['Time'].apply(lambda x: int(x.split(' ')[1].split(':')[0]))\n",
    "\n",
    "merged_dataframe['efficiency_adjusted_power'] = merged_dataframe['chiller_efficiency']\n",
    "merged_dataframe = merged_dataframe.drop('chiller_efficiency', axis=1)\n",
    "\n",
    "merged_dataframe['technological_centers_consumption'] = merged_dataframe['08 Technological Centers_Electric_Active Energy (kWh)']\n",
    "merged_dataframe = merged_dataframe.drop('08 Technological Centers_Electric_Active Energy (kWh)', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfc069",
   "metadata": {},
   "source": [
    "Check the nulls and solve by a linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52408ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataframe.isnull().sum().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87af749cf7e27295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:02:32.397577Z",
     "start_time": "2024-07-09T21:02:32.393906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataframe  = merged_dataframe.interpolate(method='linear', axis=0)\n",
    "merged_dataframe.isnull().sum().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f55cb2",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d079148ca84ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:02:32.427142Z",
     "start_time": "2024-07-09T21:02:32.418422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the result to a new CSV file\n",
    "merged_dataframe.to_csv('optimizer/optimizer_input/parameters.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
